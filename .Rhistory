select(-geometry), by = c("Origin.Tract" = "GEOID"))
# Chunk 21: time_lags
ride.panel <-
ride.panel %>%
arrange(from_station_id, interval60) %>%
mutate(lagHour = dplyr::lag(Trip_Count,1),
lag2Hours = dplyr::lag(Trip_Count,2),
lag3Hours = dplyr::lag(Trip_Count,3),
lag4Hours = dplyr::lag(Trip_Count,4),
lag12Hours = dplyr::lag(Trip_Count,12),
lag1day = dplyr::lag(Trip_Count,24),
holiday = ifelse(yday(interval60) == 148,1,0)) %>%
mutate(day = yday(interval60)) %>%
mutate(holidayLag = case_when(dplyr::lag(holiday, 1) == 1 ~ "PlusOneDay",
dplyr::lag(holiday, 2) == 1 ~ "PlustTwoDays",
dplyr::lag(holiday, 3) == 1 ~ "PlustThreeDays",
dplyr::lead(holiday, 1) == 1 ~ "MinusOneDay",
dplyr::lead(holiday, 2) == 1 ~ "MinusTwoDays",
dplyr::lead(holiday, 3) == 1 ~ "MinusThreeDays"),
holidayLag = ifelse(is.na(holidayLag) == TRUE, 0, holidayLag))
# Chunk 22: evaluate_lags
as.data.frame(ride.panel) %>%
group_by(interval60) %>%
summarise_at(vars(starts_with("lag"), "Trip_Count"), mean, na.rm = TRUE) %>%
gather(Variable, Value, -interval60, -Trip_Count) %>%
mutate(Variable = factor(Variable, levels=c("lagHour","lag2Hours","lag3Hours","lag4Hours",
"lag12Hours","lag1day")))%>%
group_by(Variable) %>%
summarize(correlation = round(cor(Value, Trip_Count),2))
# Chunk 23: train_test
ride.Train <- filter(ride.panel, week >= 20)
ride.Test <- filter(ride.panel, week < 20)
# Chunk 24: five_models
reg1 <-
lm(Trip_Count ~  hour(interval60) + dotw + Temperature,  data=ride.Train)
reg2 <-
lm(Trip_Count ~  from_station_name + dotw + Temperature,  data=ride.Train)
reg3 <-
lm(Trip_Count ~  from_station_name + hour(interval60) + dotw + Temperature + Precipitation,
data=ride.Train)
reg4 <-
lm(Trip_Count ~  from_station_name +  hour(interval60) + dotw + Temperature + Precipitation +
lagHour + lag2Hours +lag3Hours + lag12Hours + lag1day,
data=ride.Train)
reg5 <-
lm(Trip_Count ~  from_station_name + hour(interval60) + dotw + Temperature + Precipitation +
lagHour + lag2Hours +lag3Hours +lag12Hours + lag1day + holidayLag + holiday,
data=ride.Train)
# Chunk 25: nest_data
ride.Test.weekNest <-
ride.Test %>%
nest(-week)
# Chunk 26: predict_function
model_pred <- function(dat, fit){
pred <- predict(fit, newdata = dat)}
# Chunk 27: do_predicitons
week_predictions <-
ride.Test.weekNest %>%
mutate(ATime_FE = map(.x = data, fit = reg1, .f = model_pred),
BSpace_FE = map(.x = data, fit = reg2, .f = model_pred),
CTime_Space_FE = map(.x = data, fit = reg3, .f = model_pred),
DTime_Space_FE_timeLags = map(.x = data, fit = reg4, .f = model_pred),
ETime_Space_FE_timeLags_holidayLags = map(.x = data, fit = reg5, .f = model_pred)) %>%
gather(Regression, Prediction, -data, -week) %>%
mutate(Observed = map(data, pull, Trip_Count),
Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),
MAE = map_dbl(Absolute_Error, mean, na.rm = TRUE),
sd_AE = map_dbl(Absolute_Error, sd, na.rm = TRUE))
week_predictions
# Chunk 28: plot_errors_by_model
week_predictions %>%
dplyr::select(week, Regression, MAE) %>%
gather(Variable, MAE, -Regression, -week) %>%
ggplot(aes(week, MAE)) +
geom_bar(aes(fill = Regression), position = "dodge", stat="identity") +
scale_fill_manual(values = palette5) +
labs(title = "Mean Absolute Errors by model specification and week") +
plotTheme
# Chunk 29: error_vs_actual_timeseries
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id)) %>%
dplyr::select(interval60, from_station_id, Observed, Prediction, Regression) %>%
unnest() %>%
gather(Variable, Value, -Regression, -interval60, -from_station_id) %>%
group_by(Regression, Variable, interval60) %>%
summarize(Value = sum(Value)) %>%
ggplot(aes(interval60, Value, colour=Variable)) +
geom_line(size = 1.1) +
facet_wrap(~Regression, ncol=1) +
labs(title = "Predicted/Observed bike share time series", subtitle = "Chicago; A test set of 2 weeks",  x = "Hour", y= "Station Trips") +
plotTheme
# Chunk 30: errors_by_station
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude)) %>%
select(interval60, from_station_id, from_longitude, from_latitude, Observed, Prediction, Regression) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags") %>%
group_by(from_station_id, from_longitude, from_latitude) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = from_longitude, y = from_latitude, color = MAE),
fill = "transparent", alpha = 0.4)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
labs(title="Mean Abs Error, Test Set, Model 5")+
mapTheme
# Chunk 31: obs_pred_all
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
ggplot()+
geom_point(aes(x= Observed, y = Prediction))+
geom_smooth(aes(x= Observed, y= Prediction), method = "lm", se = FALSE, color = "red")+
geom_abline(slope = 1, intercept = 0)+
facet_grid(time_of_day~weekend)+
labs(title="Observed vs Predicted",
x="Observed trips",
y="Predicted trips")+
plotTheme
# Chunk 32: station_summary
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw) ) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
group_by(from_station_id, weekend, time_of_day, from_longitude, from_latitude) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = from_longitude, y = from_latitude, color = MAE),
fill = "transparent", size = 0.5, alpha = 0.4)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
facet_grid(weekend~time_of_day)+
labs(title="Mean Absolute Errors, Test Set")+
mapTheme
# Chunk 33: station_summary2
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw),
Percent_Taking_Public_Trans = map(data, pull, Percent_Taking_Public_Trans),
Med_Inc = map(data, pull, Med_Inc),
Percent_White = map(data, pull, Percent_White)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
filter(time_of_day == "AM Rush") %>%
group_by(from_station_id, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
gather(-from_station_id, -MAE, key = "variable", value = "value")%>%
ggplot(.)+
#geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = value, y = MAE), alpha = 0.4)+
geom_smooth(aes(x = value, y = MAE), method = "lm", se= FALSE)+
facet_wrap(~variable, scales = "free")+
labs(title="Errors as a function of socio-economic variables",
y="Mean Absolute Error (Trips)")+
plotTheme
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: packages
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(tidycensus)
library(viridis)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(RSocrata)
library(rjson)
library(xml2)
library(httr)
plotTheme <- theme(
plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 10),
# Set the entire chart region to blank
panel.background=element_blank(),
plot.background=element_blank(),
#panel.border=element_rect(colour="#F0F0F0"),
# Format the grid
panel.grid.major=element_line(colour="#D0D0D0",size=.2),
axis.ticks=element_blank())
mapTheme <- theme(plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank(),
panel.background=element_blank(),
panel.border=element_blank(),
panel.grid.major=element_line(colour = 'transparent'),
panel.grid.minor=element_blank(),
legend.direction = "vertical",
legend.position = "right",
plot.margin = margin(1, 1, 1, 1, 'cm'),
legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))
palette5 <- c("#eff3ff","#bdd7e7","#6baed6","#3182bd","#08519c")
palette4 <- c("#D2FBD4","#92BCAB","#527D82","#123F5A")
palette2 <- c("#6baed6","#08519c")
# Chunk 3: bike share station
response <- GET("http://www.tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml")
xml_content <- content(response, as = "text")
xml_data <- read_xml(xml_content)
print(xml_data)
stations <- xml_find_all(xml_data, "//station")
station_names <- xml_text(xml_find_all(stations, "name"))
available_docks <- xml_text(xml_find_all(stations, "nbDocks"))
terminal_names <- xml_text(xml_find_all(stations, "terminalName"))
longitudes <- xml_text(xml_find_all(stations, "long"))
latitudes <- xml_text(xml_find_all(stations, "lat"))
# Combine into a data frame
cycle_hire_data <- data.frame(
Station = station_names,
AvailableDocks = as.numeric(available_docks),
TerminalName = as.numeric(terminal_names),
Longitude = as.numeric(longitudes),
Latitude = as.numeric(latitudes)
)
view(cycle_hire_data)
# Chunk 4: bike hire
Jul_01<-read.csv("376JourneyDataExtract01Jul2023-14Jul2023.csv")
source("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/11. Week 11- Statistical and Data Mining Methods/2022 - Point Pattern Analysis.R", echo=TRUE)
install.packages("spatialEco")
library(spatstat)
library(sp)
library(fossil)
library(spatial)
library(adehabitatHR)
library(gdata)
library(raster)
library(rgdal)
# Chunk 1: setup
knitr::opts_chunk$set(
echo = TRUE,
results = 'hide',
message = FALSE,
warning = FALSE,
fig.align = 'left')
library(tidyverse)
library(extrafont) # Ubuntu
library(kableExtra)
library(readr)
library(NbClust)
library(flexclust)
library(sf)
library(ggplot2)
library(gridExtra)
palette5 <- c("#f0f9e8","#bae4bc","#7bccc4","#43a2ca","#0868ac")
flatreds5 <- c('#f9ebea','#e6b0aa','#cd6155', '#cd6155','#7b241c')
flatblues5 <- c('#ebf5fb', '#a9cce3', '#5499c7', '#2471a3',  '#1a5276')
flatsage5 <- c('#f6fff8','#eaf4f4', '#cce3de','#a4c3b2','#6b9080')
flatoranges5 <- c('#fff8e1', '#ffcc80', '#ffab40', '#ff8f00', '#ff6f00')
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
# Chunk 2: data
urlfile = "https://raw.githubusercontent.com/JiaYue-Ong/MUSA5000-Statistical-and-Data-Mining-Homework4-5/refs/heads/main/RegressionData.csv"
blockgroup <- read.csv(url(urlfile))
df<- data.frame(scale(blockgroup[-1:0]))
# Chunk 3: Scree plot
bgss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:20) bgss[i] <- sum(kmeans(df,
centers=i)$withinss)
plot(1:20, bgss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# Chunk 4: NbClust
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans", index="all")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/MUSA5000-Statistical-and-Data-Mining-Homework4-5")
shp<-st_read("RegressionData.shp")
# median price per sqft
price_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(MEDHVAL)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatblues5,
labels = qBr(shp, "MEDHVAL"),
name = 'Median Household Value') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Median household values across blocks in Philadelphia",
subtitle = "Values classified in quintiles")
vacant_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTVACANT)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatreds5,
labels = qBr(shp, "PCTVACANT"),
name = 'Single House Units (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Vacant House Units (%) ",
subtitle = "Values classified in quintiles")
singles_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTSINGLES)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatreds5,
labels = qBr(shp, "PCTSINGLES"),
name = 'Vacant House Units (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Single House Units (%) ",
subtitle = "Values classified in quintiles")
bachelor_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTBACHMOR)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatsage5,
labels = qBr(shp, "PCTBACHMOR"),
name = 'Bachelors Degree (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Population with Bachelor's degree (%)",
subtitle = "Values classified in quintiles")
poverty_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(MEDHHINC)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatblues5,
labels = qBr(shp, "MEDHHINC"),
name = 'Median Household Income') +
theme_void()+
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Median household income across blocks in Philadelphia",
subtitle = "Values classified in quintiles")
price_map
grid.arrange(poverty_map,  singles_map, bachelor_map, vacant_map,
top = textGrob("Spatial Distribution of different variables",
gp = gpar(fontsize = 11, fontface = "bold", fontfamily = "Lato")),
layout_matrix = cbind(c(1, 2),
c(1, 2),
c(1, 2),
c(3, 4 ),
c(3, 4 ),
c(3, 4 )))
grid.arrange(poverty_map,  singles_map, bachelor_map, vacant_map,
top = textGrob("Spatial Distribution of different variables",
gp = gpar(fontsize = 11, fontface = "bold", fontfamily = "Lato")),
layout_matrix = cbind(c(1, 2),
c(1, 2),
c(1, 2),
c(3, 4 ),
c(3, 4 ),
c(3, 4 )))
shp<-st_read("RegressionData.shp")
# median price per sqft
price_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(MEDHVAL)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatblues5,
labels = qBr(shp, "MEDHVAL"),
name = 'Median Household Value') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Median household values across blocks in Philadelphia",
subtitle = "Values classified in quintiles")
vacant_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTVACANT)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatreds5,
labels = qBr(shp, "PCTVACANT"),
name = 'Vacant House Units (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Vacant House Units (%) ",
subtitle = "Values classified in quintiles")
singles_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTSINGLES)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatreds5,
labels = qBr(shp, "PCTSINGLES"),
name = 'Single House Units (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Single House Units (%) ",
subtitle = "Values classified in quintiles")
bachelor_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(PCTBACHMOR)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatsage5,
labels = qBr(shp, "PCTBACHMOR"),
name = 'Bachelors Degree (%)') +
theme_void() +
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Population with Bachelor's degree (%)",
subtitle = "Values classified in quintiles")
poverty_map <- ggplot() +
geom_sf(data = shp, color = 'grey80', fill = 'grey90') +
geom_sf(data = shp, mapping=aes(fill = q5(MEDHHINC)), color = NA, alpha = 0.75)+
scale_fill_manual(values = flatblues5,
labels = qBr(shp, "MEDHHINC"),
name = 'Median Household Income') +
theme_void()+
theme(
legend.position = "right",
text = element_text(family = "Lato"),
plot.title = element_text(hjust = 0, face = "bold"),
plot.subtitle = element_text(hjust = 0)) +
labs(
title = "Median household income across blocks in Philadelphia",
subtitle = "Values classified in quintiles")
price_map
grid.arrange(poverty_map,  singles_map, bachelor_map, vacant_map,
top = textGrob("Spatial Distribution of different variables",
gp = gpar(fontsize = 11, fontface = "bold", fontfamily = "Lato")),
layout_matrix = cbind(c(1, 2),
c(1, 2),
c(1, 2),
c(3, 4 ),
c(3, 4 ),
c(3, 4 )))
grid.arrange(poverty_map,  singles_map, bachelor_map, vacant_map,
top = textGrob("Spatial Distribution of different variables",
gp = gpar(fontsize = 11, fontface = "bold", fontfamily = "Lato")),
layout_matrix = cbind(c(1, 2),
c(1, 2),
c(1, 2),
c(3, 4 ),
c(3, 4 ),
c(3, 4 )))
