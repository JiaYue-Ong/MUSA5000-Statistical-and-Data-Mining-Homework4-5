scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
labs(title="Mean Abs Error, Test Set, Model 5")+
mapTheme
# Chunk 31: obs_pred_all
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
ggplot()+
geom_point(aes(x= Observed, y = Prediction))+
geom_smooth(aes(x= Observed, y= Prediction), method = "lm", se = FALSE, color = "red")+
geom_abline(slope = 1, intercept = 0)+
facet_grid(time_of_day~weekend)+
labs(title="Observed vs Predicted",
x="Observed trips",
y="Predicted trips")+
plotTheme
# Chunk 32: station_summary
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw) ) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
group_by(from_station_id, weekend, time_of_day, from_longitude, from_latitude) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = from_longitude, y = from_latitude, color = MAE),
fill = "transparent", size = 0.5, alpha = 0.4)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
facet_grid(weekend~time_of_day)+
labs(title="Mean Absolute Errors, Test Set")+
mapTheme
# Chunk 33: station_summary2
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw),
Percent_Taking_Public_Trans = map(data, pull, Percent_Taking_Public_Trans),
Med_Inc = map(data, pull, Med_Inc),
Percent_White = map(data, pull, Percent_White)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
filter(time_of_day == "AM Rush") %>%
group_by(from_station_id, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
gather(-from_station_id, -MAE, key = "variable", value = "value")%>%
ggplot(.)+
#geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = value, y = MAE), alpha = 0.4)+
geom_smooth(aes(x = value, y = MAE), method = "lm", se= FALSE)+
facet_wrap(~variable, scales = "free")+
labs(title="Errors as a function of socio-economic variables",
y="Mean Absolute Error (Trips)")+
plotTheme
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: packages
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(tidycensus)
library(viridis)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(RSocrata)
library(rjson)
library(xml2)
library(httr)
plotTheme <- theme(
plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 10),
# Set the entire chart region to blank
panel.background=element_blank(),
plot.background=element_blank(),
#panel.border=element_rect(colour="#F0F0F0"),
# Format the grid
panel.grid.major=element_line(colour="#D0D0D0",size=.2),
axis.ticks=element_blank())
mapTheme <- theme(plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank(),
panel.background=element_blank(),
panel.border=element_blank(),
panel.grid.major=element_line(colour = 'transparent'),
panel.grid.minor=element_blank(),
legend.direction = "vertical",
legend.position = "right",
plot.margin = margin(1, 1, 1, 1, 'cm'),
legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))
palette5 <- c("#eff3ff","#bdd7e7","#6baed6","#3182bd","#08519c")
palette4 <- c("#D2FBD4","#92BCAB","#527D82","#123F5A")
palette2 <- c("#6baed6","#08519c")
# Chunk 3: bike share station
response <- GET("http://www.tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml")
xml_content <- content(response, as = "text")
xml_data <- read_xml(xml_content)
print(xml_data)
stations <- xml_find_all(xml_data, "//station")
station_names <- xml_text(xml_find_all(stations, "name"))
available_docks <- xml_text(xml_find_all(stations, "nbDocks"))
terminal_names <- xml_text(xml_find_all(stations, "terminalName"))
longitudes <- xml_text(xml_find_all(stations, "long"))
latitudes <- xml_text(xml_find_all(stations, "lat"))
# Combine into a data frame
cycle_hire_data <- data.frame(
Station = station_names,
AvailableDocks = as.numeric(available_docks),
TerminalName = as.numeric(terminal_names),
Longitude = as.numeric(longitudes),
Latitude = as.numeric(latitudes)
)
view(cycle_hire_data)
# Chunk 4: bike hire
Jul_01<-read.csv("376JourneyDataExtract01Jul2023-14Jul2023.csv")
source("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/11. Week 11- Statistical and Data Mining Methods/2022 - Point Pattern Analysis.R", echo=TRUE)
install.packages("spatialEco")
library(spatstat)
library(sp)
library(fossil)
library(spatial)
library(adehabitatHR)
library(gdata)
library(raster)
library(rgdal)
setwd("./MUSA5000-Statistical-and-Data-Mining-Homework4-5")
knitr::opts_chunk$set(echo = TRUE,
results = 'hide',
message = FALSE,
warning = FALSE,
fig.align = 'left')
library(spatstat)
library(spatstat.explore)
library(sf)
Philly<- st_read("./HW4/Philadelphia.shp")
knitr::opts_chunk$set(echo = TRUE,
results = 'hide',
message = FALSE,
warning = FALSE,
fig.align = 'left')
library(spatstat)
library(spatstat.explore)
library(sf)
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/Stats HW4")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/Stats HW4")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/Stats HW4")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/Stats HW4/HW 4")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/Stats HW4")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis")
Philly<- st_read("./HW4/Philadelphia.shp")
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/MUSA5000-Statistical-and-Data-Mining-Homework4-5")
Philly<- st_read("./HW4/Philadelphia.shp")
Markets<-st_read("./HW4/Philadelphia_Farmers_Markets201302.shp")
Philly_Zip<-st_read("./HW4/Philadelphia_ZipCodes.shp")
# Plot points and boundary
ggplot() +
geom_sf(data = Philly_Zip, fill = "lightblue", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
library(ggplot2)
# Plot points and boundary
ggplot() +
geom_sf(data = Philly_Zip, fill = "lightblue", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
# Plot points and boundary
ggplot() +
geom_sf(data = Philly, fill = "lightblue", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
View(Philly_Zip)
ggplot() +
geom_sf(data = Philly_Zip, fill = "MedHHINc", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
View(Philly_Zip)
ggplot() +
geom_sf(data = Philly_Zip, fill = MedIncome, color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
scale_fill_viridis_c()+
theme_minimal()
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
scale_fill_viridis_c(option = "cividis")+
theme_minimal()
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
scale_fill_viridis_c(option = "inferno")+
theme_minimal()
cividis
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
scale_fill_viridis_c(option = "cividis")+
theme_minimal()
#
BoundaryPolygons <- as(Philly, "SpatialPolygons")
#
BoundaryPolygons <- as_Spatial(Philly)
View(BoundaryPolygons)
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")
BoundaryPolygonsOW<- as.owin(BoundaryPolygons)
View(BoundaryPolygons)
library(maptools)
install.packages(maptools)
install.packages("maptools")
library(spatstat.geom)
BoundaryPolygonsOW<- as.owin(BoundaryPolygons)
BoundaryPolygonsOW<- as(BoundaryPolygons,"owin")
BoundaryPolygonsOnly <- as(BoundaryPolygons, "SpatialPolygons")
BoundaryPolygonsOW<- as(BoundaryPolygonsOnly,"owin")
BoundaryPolygonsOW<- as.owin(BoundaryPolygonsOnly)
BoundaryPolygonsOnly <- as(BoundaryPolygons, "Spatial")
BoundaryPolygonsOW<- as.owin(BoundaryPolygonsOnly)
#
BoundaryPolygons <- as_Spatial(Philly)
BoundaryPolygons <- st_polygonize(st_as_sf(BoundaryPolygons))
BoundaryPolygons <- st_polygonize(st_as_sf(Philly))
BoundaryPolygons <- as(Philly, "SpatialPolygons")
#
Boundary <- as_Spatial(Philly)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")
#
aux <- readShapeSpatial("./HW4/Philadelphia.shp")
#
aux <- st_polygonize(st_as_sf(Philly))
aux <- as(Philly, "Spatial")
city <- as.owin(aux)
city <- as.owin(BoundaryPolygons)
BoundaryPolygonsOW<- as(Philly, "owin")
remove(aux)
#
boundary.w <- as.owin(as_Spatial(Philly))
#
boundary.w <- as(as_Spatial(boundary.shp),"owin")
#
boundary.w <- as(as_Spatial(Philly),"owin")
?as.owin.sf
??as.owin.sf
methods(as.owin)
Boundary <- as.owin.sf(Philly)
View(Philly)
# Boundary
Boundary <- as_Spatial(Philly)
View(Boundary)
# Boundary
Boundary <- as_Spatial(Philly$geometry)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")
View(BoundaryPolygons)
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")
# Boundary
Boundary <- as_Spatial(Philly)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")
# Create a ppp
pp <-as.ppp(Markets, window=BoundaryPolygonsOW)
View(pp)
# Create a ppp
pp <-as.ppp(Markets)
# Coordinates
coords<- st_coordinates(Markets)
# Coordinates
coords<- col(st_coordinates(Markets))
# Coordinates
coords<- c(st_coordinates(Markets))
# Coordinates
coords<- data.frame(st_coordinates(Markets))
View(coords)
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")
BoundaryPolygonsOW<- as.owin.SpatialPolygons(BoundaryPolygons)
BoundaryPolygonsOW<- owin(ploy=BoundaryPolygons)
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
owin
# Coordinates
coords<- data.frame(st_coordinates(Markets))
pp2 <- ppp(coords$X, coords$Y, window=BoundaryPolygonsOW)
View(pp2)
BoundaryPolygonsOW<- owin(poly=BoundaryPolygons)
BoundaryPolygonsOW<- owin(BoundaryPolygons)
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
remove(BoundaryPolygonsOW)
BoundaryPolygonsOW<- owin(BoundaryPolygons)
BoundaryPolygonsOW<- window(BoundaryPolygons)
plot(BoundaryPolygonsOW, main=NULL)
pp2 <- ppp(coords$X, coords$Y, window=BoundaryPolygonsOW)
pp2 <- ppp(coords$X, coords$Y, window=BoundaryPolygons)
BoundaryPolygonsOW<- owin(poly=BoundaryPolygons)
BoundaryPolygonsOW<- as.owin(BoundaryPolygons)
BoundaryPolygons2<-st_sfc(BoundaryPolygons)
philly_bound <- as.owin(Philly)
# Boundary
Boundary <- as.owin(Philly)
# Boundary
BoundaryPolygonsOW <- as.owin(Philly)
#Plotting the boundary window
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
# Coordinates
coords<- data.frame(st_coordinates(Markets))
pp2 <- ppp(coords$X, coords$Y, window=BoundaryPolygonsOW)
View(coords)
cbind(coords,duplicated(X,Y))
colnames(coords)
duplicated(X,Y)
duplicated(coords$X,coords$Y)
cbind(coords,duplicated(coords$X,coords$Y))
library(dplyr)
coords2<-coords %>% dplyrs::distinct(X, Y, .keep_all = TRUE)
coords2<-coords %>% dplyr::distinct(X, Y, .keep_all = TRUE)
coords2<-coords %>% coords[!duplicated(coords), ]
coords2<-coords %>% coords[!duplicated(coords[, c("X", "Y")]), ]
coords2<-coords[!duplicated(coords), ]
# Check for duplicated values
cbind(coords2,duplicated(coords2$X,coords2$Y))
plot(pp,add=T)
# Points
pp2 <- ppp(coords2$X, coords2$Y, window=BoundaryPolygonsOW)
plot(pp2,add=T)
plot(pp2,add=T)
plot(pp,add=T)
plot(pp,add=T)
plot(pp2,add=T)
# Points
pp2 <- ppp(coords2$X, coords2$Y, window=BoundaryPolygonsOW)
plot(pp2,add=T)
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
scale_fill_viridis_c(option = "cividis")+
theme_minimal()
# Boundary
BoundaryPolygonsOW <- as.owin(Philly)
#Plotting the boundary window
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
# Coordinates
coords<- data.frame(st_coordinates(Markets))
# Check for duplicated values
cbind(coords,duplicated(coords$X,coords$Y))
coords2<-coords[!duplicated(coords), ]
# Points
pp2 <- ppp(coords2$X, coords2$Y, window=BoundaryPolygonsOW)
plot(pp2,add=T)
#Plotting the boundary window
plot(BoundaryPolygonsOW, main=NULL)
plot(pp,add=T)
dev.off()
#Computes the distance from each point to its nearest neighbour in a point pattern.
nnd <- nndist.ppp(pp2)
#Using the formulas on the slides, we calculate Mean Observed Distance,
#Mean Expected Distance and the Standard Error.
MeanObsDist <- mean(nnd)
#The area.owin command calculates the area of the study area that you use. Here it's the minimum enclosing rectangle, but it doesn't have to be - it could be any shapefile you import from ArcGIS (or generate in R) that corresponds to the study area.
MeanExpDist <- 0.5 / sqrt(nrow(coords2) / area.owin(BoundaryPolygonsOW))
SE <- 0.26136 / sqrt(nrow(coords2)*nrow(coords2) / area.owin(BoundaryPolygonsOW))
#Calculating the z-score
zscore <- (MeanObsDist - MeanExpDist)/SE
#Statistical test
#Here, if the z score is positive, we do an upper-tailed test and if the z score is negative we do a lower-tailed test to come up with the p-value.
pval<-ifelse(zscore > 0, 1 - pnorm(zscore), pnorm(zscore))
#Calculating the NNI
NNI <- MeanObsDist / MeanExpDist
pval
NNI
# Create a ppp
pp <-as.ppp(Markets)
#Computes the distance from each point to its nearest neighbour in a point pattern.
nnd <- nndist.ppp(pp)
#Using the formulas on the slides, we calculate Mean Observed Distance,
#Mean Expected Distance and the Standard Error.
MeanObsDist <- mean(nnd)
#The st_area command calculates the area of the study area that you use.
#Here it's the minimum enclosing rectangle, but it doesn't have to be - it
#could be any shapefile you import from ArcGIS (or generate in R) that
#corresponds to the study area.
MeanExpDist <-as.numeric(
0.5 / sqrt(nrow(Markets) / st_area(Philly))
)
SE <- as.numeric(
0.26136 / sqrt(nrow(Markets)*nrow(Markets) / st_area(Philly))
)
#Calculating the z-score
zscore <- (MeanObsDist - MeanExpDist)/SE
#Statistical test
#Here, if the z score is positive, we do an upper-tailed test and if the
#z score is negative we do a lower-tailed test to come up with the p-value.
pval<-ifelse(zscore > 0, 1 - pnorm(zscore), pnorm(zscore))
#Calculating the NNI
NNI <- MeanObsDist / MeanExpDist
pval
NNI
khat <-Kest(pp2, rmax=250000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.core::envelope(pp,fun="Kest", rmax=250000, nsim=9, nrank=1)
khat <-Kest(pp2, rmax=250000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::::envelope(pp2,fun="Kest", rmax=250000, nsim=9, nrank=1)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::envelope(pp2,fun="Kest", rmax=250000, nsim=9, nrank=1)
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main=
"Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
#If we double click on the khat data set on the right, it will have 513 observations and 5 variables. We are interested in 2 of the variables:
#-- r, which is the distance that goes in increments of 138.8693
#-- iso, which is the k-function calculated with Ripley's edge correction
#K-Functions
khat <-Kest(pp, rmax=250000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::envelope(pp,fun="Kest", rmax=250000, nsim=9, nrank=1)
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main=
"Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
# Plot points and boundary
ggplot() +
geom_sf(data = Philly, fill = "lightblue", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
