weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
group_by(from_station_id, from_latitude, from_longitude, weekend, time_of_day) %>%
tally(),
aes(x=from_longitude, y = from_latitude, color = n),
fill = "transparent", alpha = 0.4, size = 0.3)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
facet_grid(weekend ~ time_of_day)+
labs(title="Bike share trips per hr by station. Chicago, May, 2018")+
mapTheme
# Chunk 18: panel_length_check
length(unique(dat_census$interval60)) * length(unique(dat_census$from_station_id))
study.panel <-
expand.grid(interval60=unique(dat_census$interval60),
from_station_id = unique(dat_census$from_station_id)) %>%
left_join(., dat_census %>%
select(from_station_id, from_station_name, Origin.Tract, from_longitude, from_latitude )%>%
distinct() %>%
group_by(from_station_id) %>%
slice(1))
nrow(study.panel)
# Chunk 19: create_panel
ride.panel <-
dat_census %>%
mutate(Trip_Counter = 1) %>%
right_join(study.panel) %>%
group_by(interval60, from_station_id, from_station_name, Origin.Tract, from_longitude, from_latitude) %>%
summarize(Trip_Count = sum(Trip_Counter, na.rm=T)) %>%
left_join(weather.Panel) %>%
ungroup() %>%
filter(is.na(from_station_id) == FALSE) %>%
mutate(week = week(interval60),
dotw = wday(interval60, label = TRUE)) %>%
filter(is.na(Origin.Tract) == FALSE)
# Chunk 20: census_and_panel
ride.panel <-
left_join(ride.panel, chicagoCensus %>%
as.data.frame() %>%
select(-geometry), by = c("Origin.Tract" = "GEOID"))
# Chunk 21: time_lags
ride.panel <-
ride.panel %>%
arrange(from_station_id, interval60) %>%
mutate(lagHour = dplyr::lag(Trip_Count,1),
lag2Hours = dplyr::lag(Trip_Count,2),
lag3Hours = dplyr::lag(Trip_Count,3),
lag4Hours = dplyr::lag(Trip_Count,4),
lag12Hours = dplyr::lag(Trip_Count,12),
lag1day = dplyr::lag(Trip_Count,24),
holiday = ifelse(yday(interval60) == 148,1,0)) %>%
mutate(day = yday(interval60)) %>%
mutate(holidayLag = case_when(dplyr::lag(holiday, 1) == 1 ~ "PlusOneDay",
dplyr::lag(holiday, 2) == 1 ~ "PlustTwoDays",
dplyr::lag(holiday, 3) == 1 ~ "PlustThreeDays",
dplyr::lead(holiday, 1) == 1 ~ "MinusOneDay",
dplyr::lead(holiday, 2) == 1 ~ "MinusTwoDays",
dplyr::lead(holiday, 3) == 1 ~ "MinusThreeDays"),
holidayLag = ifelse(is.na(holidayLag) == TRUE, 0, holidayLag))
# Chunk 22: evaluate_lags
as.data.frame(ride.panel) %>%
group_by(interval60) %>%
summarise_at(vars(starts_with("lag"), "Trip_Count"), mean, na.rm = TRUE) %>%
gather(Variable, Value, -interval60, -Trip_Count) %>%
mutate(Variable = factor(Variable, levels=c("lagHour","lag2Hours","lag3Hours","lag4Hours",
"lag12Hours","lag1day")))%>%
group_by(Variable) %>%
summarize(correlation = round(cor(Value, Trip_Count),2))
# Chunk 23: train_test
ride.Train <- filter(ride.panel, week >= 20)
ride.Test <- filter(ride.panel, week < 20)
# Chunk 24: five_models
reg1 <-
lm(Trip_Count ~  hour(interval60) + dotw + Temperature,  data=ride.Train)
reg2 <-
lm(Trip_Count ~  from_station_name + dotw + Temperature,  data=ride.Train)
reg3 <-
lm(Trip_Count ~  from_station_name + hour(interval60) + dotw + Temperature + Precipitation,
data=ride.Train)
reg4 <-
lm(Trip_Count ~  from_station_name +  hour(interval60) + dotw + Temperature + Precipitation +
lagHour + lag2Hours +lag3Hours + lag12Hours + lag1day,
data=ride.Train)
reg5 <-
lm(Trip_Count ~  from_station_name + hour(interval60) + dotw + Temperature + Precipitation +
lagHour + lag2Hours +lag3Hours +lag12Hours + lag1day + holidayLag + holiday,
data=ride.Train)
# Chunk 25: nest_data
ride.Test.weekNest <-
ride.Test %>%
nest(-week)
# Chunk 26: predict_function
model_pred <- function(dat, fit){
pred <- predict(fit, newdata = dat)}
# Chunk 27: do_predicitons
week_predictions <-
ride.Test.weekNest %>%
mutate(ATime_FE = map(.x = data, fit = reg1, .f = model_pred),
BSpace_FE = map(.x = data, fit = reg2, .f = model_pred),
CTime_Space_FE = map(.x = data, fit = reg3, .f = model_pred),
DTime_Space_FE_timeLags = map(.x = data, fit = reg4, .f = model_pred),
ETime_Space_FE_timeLags_holidayLags = map(.x = data, fit = reg5, .f = model_pred)) %>%
gather(Regression, Prediction, -data, -week) %>%
mutate(Observed = map(data, pull, Trip_Count),
Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),
MAE = map_dbl(Absolute_Error, mean, na.rm = TRUE),
sd_AE = map_dbl(Absolute_Error, sd, na.rm = TRUE))
week_predictions
# Chunk 28: plot_errors_by_model
week_predictions %>%
dplyr::select(week, Regression, MAE) %>%
gather(Variable, MAE, -Regression, -week) %>%
ggplot(aes(week, MAE)) +
geom_bar(aes(fill = Regression), position = "dodge", stat="identity") +
scale_fill_manual(values = palette5) +
labs(title = "Mean Absolute Errors by model specification and week") +
plotTheme
# Chunk 29: error_vs_actual_timeseries
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id)) %>%
dplyr::select(interval60, from_station_id, Observed, Prediction, Regression) %>%
unnest() %>%
gather(Variable, Value, -Regression, -interval60, -from_station_id) %>%
group_by(Regression, Variable, interval60) %>%
summarize(Value = sum(Value)) %>%
ggplot(aes(interval60, Value, colour=Variable)) +
geom_line(size = 1.1) +
facet_wrap(~Regression, ncol=1) +
labs(title = "Predicted/Observed bike share time series", subtitle = "Chicago; A test set of 2 weeks",  x = "Hour", y= "Station Trips") +
plotTheme
# Chunk 30: errors_by_station
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude)) %>%
select(interval60, from_station_id, from_longitude, from_latitude, Observed, Prediction, Regression) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags") %>%
group_by(from_station_id, from_longitude, from_latitude) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = from_longitude, y = from_latitude, color = MAE),
fill = "transparent", alpha = 0.4)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
labs(title="Mean Abs Error, Test Set, Model 5")+
mapTheme
# Chunk 31: obs_pred_all
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush"))%>%
ggplot()+
geom_point(aes(x= Observed, y = Prediction))+
geom_smooth(aes(x= Observed, y= Prediction), method = "lm", se = FALSE, color = "red")+
geom_abline(slope = 1, intercept = 0)+
facet_grid(time_of_day~weekend)+
labs(title="Observed vs Predicted",
x="Observed trips",
y="Predicted trips")+
plotTheme
# Chunk 32: station_summary
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw) ) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
group_by(from_station_id, weekend, time_of_day, from_longitude, from_latitude) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
ggplot(.)+
geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = from_longitude, y = from_latitude, color = MAE),
fill = "transparent", size = 0.5, alpha = 0.4)+
scale_colour_viridis(direction = -1,
discrete = FALSE, option = "D")+
ylim(min(dat_census$from_latitude), max(dat_census$from_latitude))+
xlim(min(dat_census$from_longitude), max(dat_census$from_longitude))+
facet_grid(weekend~time_of_day)+
labs(title="Mean Absolute Errors, Test Set")+
mapTheme
# Chunk 33: station_summary2
week_predictions %>%
mutate(interval60 = map(data, pull, interval60),
from_station_id = map(data, pull, from_station_id),
from_latitude = map(data, pull, from_latitude),
from_longitude = map(data, pull, from_longitude),
dotw = map(data, pull, dotw),
Percent_Taking_Public_Trans = map(data, pull, Percent_Taking_Public_Trans),
Med_Inc = map(data, pull, Med_Inc),
Percent_White = map(data, pull, Percent_White)) %>%
select(interval60, from_station_id, from_longitude,
from_latitude, Observed, Prediction, Regression,
dotw, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
unnest() %>%
filter(Regression == "ETime_Space_FE_timeLags_holidayLags")%>%
mutate(weekend = ifelse(dotw %in% c("Sun", "Sat"), "Weekend", "Weekday"),
time_of_day = case_when(hour(interval60) < 7 | hour(interval60) > 18 ~ "Overnight",
hour(interval60) >= 7 & hour(interval60) < 10 ~ "AM Rush",
hour(interval60) >= 10 & hour(interval60) < 15 ~ "Mid-Day",
hour(interval60) >= 15 & hour(interval60) <= 18 ~ "PM Rush")) %>%
filter(time_of_day == "AM Rush") %>%
group_by(from_station_id, Percent_Taking_Public_Trans, Med_Inc, Percent_White) %>%
summarize(MAE = mean(abs(Observed-Prediction), na.rm = TRUE))%>%
gather(-from_station_id, -MAE, key = "variable", value = "value")%>%
ggplot(.)+
#geom_sf(data = chicagoCensus, color = "grey", fill = "transparent")+
geom_point(aes(x = value, y = MAE), alpha = 0.4)+
geom_smooth(aes(x = value, y = MAE), method = "lm", se= FALSE)+
facet_wrap(~variable, scales = "free")+
labs(title="Errors as a function of socio-economic variables",
y="Mean Absolute Error (Trips)")+
plotTheme
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: packages
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(tidycensus)
library(viridis)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(RSocrata)
library(rjson)
library(xml2)
library(httr)
plotTheme <- theme(
plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 10),
# Set the entire chart region to blank
panel.background=element_blank(),
plot.background=element_blank(),
#panel.border=element_rect(colour="#F0F0F0"),
# Format the grid
panel.grid.major=element_line(colour="#D0D0D0",size=.2),
axis.ticks=element_blank())
mapTheme <- theme(plot.title =element_text(size=12),
plot.subtitle = element_text(size=8),
plot.caption = element_text(size = 6),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank(),
panel.background=element_blank(),
panel.border=element_blank(),
panel.grid.major=element_line(colour = 'transparent'),
panel.grid.minor=element_blank(),
legend.direction = "vertical",
legend.position = "right",
plot.margin = margin(1, 1, 1, 1, 'cm'),
legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))
palette5 <- c("#eff3ff","#bdd7e7","#6baed6","#3182bd","#08519c")
palette4 <- c("#D2FBD4","#92BCAB","#527D82","#123F5A")
palette2 <- c("#6baed6","#08519c")
# Chunk 3: bike share station
response <- GET("http://www.tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml")
xml_content <- content(response, as = "text")
xml_data <- read_xml(xml_content)
print(xml_data)
stations <- xml_find_all(xml_data, "//station")
station_names <- xml_text(xml_find_all(stations, "name"))
available_docks <- xml_text(xml_find_all(stations, "nbDocks"))
terminal_names <- xml_text(xml_find_all(stations, "terminalName"))
longitudes <- xml_text(xml_find_all(stations, "long"))
latitudes <- xml_text(xml_find_all(stations, "lat"))
# Combine into a data frame
cycle_hire_data <- data.frame(
Station = station_names,
AvailableDocks = as.numeric(available_docks),
TerminalName = as.numeric(terminal_names),
Longitude = as.numeric(longitudes),
Latitude = as.numeric(latitudes)
)
view(cycle_hire_data)
# Chunk 4: bike hire
Jul_01<-read.csv("376JourneyDataExtract01Jul2023-14Jul2023.csv")
source("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/11. Week 11- Statistical and Data Mining Methods/2022 - Point Pattern Analysis.R", echo=TRUE)
install.packages("spatialEco")
library(spatstat)
library(sp)
library(fossil)
library(spatial)
library(adehabitatHR)
library(gdata)
library(raster)
library(rgdal)
setwd("~/02. MUSA Year 1/MUSA 5000- Statistical And Data Mining Methods For Urban Data Analysis/MUSA5000-Statistical-and-Data-Mining-Homework4-5")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,
results = 'hide',
message = FALSE,
warning = FALSE,
fig.align = 'left')
library(spatstat)
library(spatstat.explore)
library(spatstat.geom)
library(sf)
library(ggplot2)
library(dplyr)
# Chunk 2: import data
Philly<- st_read("./HW4/Philadelphia.shp")
Markets<-st_read("./HW4/Philadelphia_Farmers_Markets201302.shp")
Philly_Zip<-st_read("./HW4/Philadelphia_ZipCodes.shp")
# Chunk 3
# Plot points and boundary
ggplot() +
geom_sf(data = Philly, fill = "lightblue", color = "black") +
geom_sf(data = Markets, color = "red", size = 2) +
theme_minimal()
# Chunk 4: Eugene's Code
# Boundary
BoundaryPolygonsOW <- as.owin(Philly)
#Plotting the boundary window
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
# Coordinates
coords<- data.frame(st_coordinates(Markets))
# Check for duplicated values
cbind(coords,duplicated(coords$X,coords$Y))
coords2<-coords[!duplicated(coords), ]
# Points
pp2 <- ppp(coords2$X, coords2$Y, window=BoundaryPolygonsOW)
plot(pp2,add=T)
# Chunk 5: Nearest Neighbor
# dev.off()
#Computes the distance from each point to its nearest neighbour in a point pattern.
nnd <- nndist.ppp(pp2)
#Using the formulas on the slides, we calculate Mean Observed Distance,
#Mean Expected Distance and the Standard Error.
MeanObsDist <- mean(nnd)
#The area.owin command calculates the area of the study area that you use. Here it's the minimum enclosing rectangle, but it doesn't have to be - it could be any shapefile you import from ArcGIS (or generate in R) that corresponds to the study area.
MeanExpDist <- 0.5 / sqrt(nrow(coords2) / area.owin(BoundaryPolygonsOW))
SE <- 0.26136 / sqrt(nrow(coords2)*nrow(coords2) / area.owin(BoundaryPolygonsOW))
#Calculating the z-score
zscore <- (MeanObsDist - MeanExpDist)/SE
#Statistical test
#Here, if the z score is positive, we do an upper-tailed test and if the z score is negative we do a lower-tailed test to come up with the p-value.
pval<-ifelse(zscore > 0, 1 - pnorm(zscore), pnorm(zscore))
#Calculating the NNI
NNI <- MeanObsDist / MeanExpDist
pval
NNI
khat <-Kest(pp2, rmax=75000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::envelope(pp2,fun="Kest", nsim=99, nrank=1)
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main=
"Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
#Computes Ripley's L* for each sample event
lhat <- Lest(pp2, rmax=75000)
#Plots Ripley's L function calculated with line width 2, Ripley's isotropic edge correction, with axis labels, and a main title.
plot(lhat$r,lhat$iso-lhat$r, xlab="r",ylab="Ripley's L",cex.lab=1.6,
cex.axis=1.5,cex.main=1.5,lty=1,lwd=2, main="Ripley's Estimated L-Function")
#Overlays the theoretical L-function under CSR with a dashed (lty=8) line.
lines(lhat$r,lhat$theo-lhat$r,lty=8,lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.explore::envelope(pp2,fun="Lest", rmax=250000, nsim=9,nrank=1)
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(Lenv,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#A better way to view this is to rotate this plot 45 degrees clockwise.
#Gives the Ripley's data frame a new name L2.
L2 <- Lenv
#Now we will subtract the distance r from the R-defined Ripley's L's
#(This will be done for the observed L, theoretical L, lower and uper envelopes)
L2$obs <- L2$obs-L2$r
L2$theo <- L2$theo-L2$r
L2$lo <- L2$lo-L2$r
L2$hi <- L2$hi-L2$r
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(L2,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.explore::envelope(pp2,fun="Lest", nsim=99,nrank=1)
#Computes Ripley's L* for each sample event
lhat <- Lest(pp2, rmax=75000)
#Plots Ripley's L function calculated with line width 2, Ripley's isotropic edge correction, with axis labels, and a main title.
plot(lhat$r,lhat$iso-lhat$r, xlab="r",ylab="Ripley's L",cex.lab=1.6,
cex.axis=1.5,cex.main=1.5,lty=1,lwd=2, main="Ripley's Estimated L-Function")
#Overlays the theoretical L-function under CSR with a dashed (lty=8) line.
lines(lhat$r,lhat$theo-lhat$r,lty=8,lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.explore::envelope(pp2,fun="Lest", rmax=250000, nsim=99,nrank=1)
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(Lenv,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#A better way to view this is to rotate this plot 45 degrees clockwise.
#Gives the Ripley's data frame a new name L2.
L2 <- Lenv
#Now we will subtract the distance r from the R-defined Ripley's L's
#(This will be done for the observed L, theoretical L, lower and uper envelopes)
L2$obs <- L2$obs-L2$r
L2$theo <- L2$theo-L2$r
L2$lo <- L2$lo-L2$r
L2$hi <- L2$hi-L2$r
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(L2,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
# Chunk 6: K-Function Analysis
khat <-Kest(pp2, rmax=75000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::envelope(pp2,fun="Kest",rmax=25000, nsim=99, nrank=1)
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main=
"Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
# Chunk 7: L-Function Analysis
#Computes Ripley's L* for each sample event
lhat <- Lest(pp2, rmax=75000)
#Plots Ripley's L function calculated with line width 2, Ripley's isotropic edge correction, with axis labels, and a main title.
plot(lhat$r,lhat$iso-lhat$r, xlab="r",ylab="Ripley's L",cex.lab=1.6,
cex.axis=1.5,cex.main=1.5,lty=1,lwd=2, main="Ripley's Estimated L-Function")
#Overlays the theoretical L-function under CSR with a dashed (lty=8) line.
lines(lhat$r,lhat$theo-lhat$r,lty=8,lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.explore::envelope(pp2,fun="Lest", rmax=25000, nsim=99,nrank=1)
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(Lenv,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#A better way to view this is to rotate this plot 45 degrees clockwise.
#Gives the Ripley's data frame a new name L2.
L2 <- Lenv
#Now we will subtract the distance r from the R-defined Ripley's L's
#(This will be done for the observed L, theoretical L, lower and uper envelopes)
L2$obs <- L2$obs-L2$r
L2$theo <- L2$theo-L2$r
L2$lo <- L2$lo-L2$r
L2$hi <- L2$hi-L2$r
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(L2,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
# Chunk 8: Map
ggplot() +
geom_sf(data = Philly_Zip, aes(fill = MedIncome), color = "black") +
geom_sf(data = Markets, color = "blue", size = 2) +
scale_fill_viridis_c(option = "cividis")+
theme_minimal()
khat <-Kest(pp2, rmax=25000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
main="Ripley's Estimated K-Function",
cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2)
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're looking at the lowest and highest values of the simulated envelopes. Here, alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat library and not the boot library.
Kenv <- spatstat.explore::envelope(pp2,fun="Kest",rmax=25000, nsim=99, nrank=1)
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main=
"Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
